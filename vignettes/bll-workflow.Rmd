---
title: "BLL Workflow"
author: "Yulric Sequeira"
date: '2022-04-29'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The aim of the Big Life Lab (BLL) workflow is to make analysis better by making it more transparent, reproducible, and reusable. Going into depth what each of those words mean from a BLL perspective:

* Transparent: An individual outside of the study team should be able to know exactly what was done at each stage of the study.
* Repriducible: An outside individual should be able to easily run your analysis on the same dataset or another database.
* Reusable: As much as possible, analysis code should be reused between different studies.

The BLL workflow aims to meet these objectives by using the following components:

* variables and variable details sheets: The varibles sheet includes all the variables that are used in the study and provides a role(s) to each one to mak it apparent where they're used in the study. The variable details sheet includes all the rules needed to derive these variables. More details for each can be seen [here](https://big-life-lab.github.io/cchsflow/articles/variables_sheet.html) and [here](https://big-life-lab.github.io/cchsflow/articles/variable_details.html). These make the study transparent, reproducible, and reusable by telling an outside individual what variables and why they've been included in the study, explicitly encoding how the variables were created, and finally allowing these rules to be reused accross difference studies on the same database.
* targets: The targets R library makes analysis more transparent and reproducible by forcing users to explicitly each step in their study using their functions. It is a workflow in its own right. It comes at a cost of extra code that needs to be written but enables analysis code to run faster. More details can be seen [here](https://books.ropensci.org/targets/).
* R Functions: FUnctions are a programming construct whose primary purpose is to avoid copy-pasting code to do the same thing accross a project(s). As much as possible, each step in the targets object calls an R function. By doing this, we are on the path to making certain portions of our analysis to be reusable in a different study. Another feature of these functions is that, as much as possible we never directly reference variables names but rather pull names out of the variables sheet using a role column. This makes it even easier for a function to be reused accross difference studies. Transparency and reproducibility can also be achieved by properly documenting and commenting the functions.

Let's see how these different components interact with each other by using a sample study.

The aim of this study will be to develope a predictive algorithm for diabetes. The outcome of the model is "risk of developing diabetes". We will be using data from the public use version of the Canadian Communit Health Survey (CCHS). The CCHS is run in cycles with data released every two years and now every year. We will be using cycles 2001, 2003 and 2005 for this study,

The first step in the workflow is to create the variables sheet specific to the study. For this study, we will use the variables sheet defined in cchsflow and remove all rows that are not applicable to this study. Our outcome is the diabetes variable and we will select two covariates, age and sex. Let's write the targets code that imports the sheets from cchsflow, and creates the variables sheet specific to our study.

```{r Create workflow sheets}
# This will create a _targets.R file in the current working directory. In an
# actual project, we would not need this since we write the code above
# in a _targets.R file in the project root.
targets::tar_script(
  list(
    # The code below tells targets to watch the variable details file for changes,
    # making sure to run it if we've updated it
    targets::tar_target(
      variable_details_sheet_file,
      "../inst/extdata/bll-workflow/variable_details.csv",
      format = "file"
    ),
    # Read and store the variable details file into an R data frame
    targets::tar_target(
      variable_details_sheet,
      read.csv(variable_details_sheet_file, fileEncoding = "UTF-8-BOM")
    ),
    # The code below tells targets to watch the variables sheet for changes,
    # making sure to run it if we've updated it
    targets::tar_target(
      variables_sheet_file,
      "../inst/extdata/bll-workflow/variables.csv",
      format = "file"
    ),
    # Read and store the variables file into an R data frame
    targets::tar_target(
      variables_sheet,
      read.csv(variables_sheet_file, fileEncoding = "UTF-8-BOM")
    )
  ),
  ask = FALSE
)

# Run the code
targets::tar_make()
```

All our worksheets have now been imported and we can access them using the `targets::tar_read` command. Lets take a look at our variables sheet.

```{r Display variables sheet}
DT::datatable(targets::tar_read("variables_sheet"))
```

As mentioned previously, it has three rows, one row with our outcome variable, diabetes, and two rows with our predictor, sec and age. Notice the role column and its value for each row, it makes it very clear why each variables was included in the study. 

Now that we have pre-specified all our variables and have all our sheets set, lets create the dataset for our study. One advantage of having a variables and variable details sheet is that we can use the recodeflow library to easily create our dataset.

```{r Create dataset}
targets::tar_script({
  # Import all the libraries we will need
  library(magrittr)
  library(dplyr)
  # We try to group steps that are used to do a particular part of the analysis.
  # For eg., above we had a group of steps used to import all the worksheets
  # that the analysis needed. We call such a group of steps a module. We declare
  # a module as a list of tar_targets and then set them to a variable. At the
  # end we will combine all the modules to create the master analysis plan
  # for our study.
  import_worksheets <- list(
    targets::tar_target(
      variable_details_sheet_file,
      "../inst/extdata/bll-workflow/variable_details.csv",
      format = "file"
    ),
    targets::tar_target(
      variable_details_sheet,
      read.csv(variable_details_sheet_file, fileEncoding = "UTF-8-BOM")
    ),
    targets::tar_target(
      variables_sheet_file,
      "../inst/extdata/bll-workflow/variables.csv",
      format = "file"
    ),
    targets::tar_target(
      variables_sheet,
      read.csv(variables_sheet_file, fileEncoding = "UTF-8-BOM")
    )
  )
  # This module will be used to create the dataset for the study
  create_study_dataset <- list(
    # The next three steps will use the recodeflow library to create the
    # starting variables from each cycle of the CCHS dataset
    targets::tar_target(
      cchs2001_p_data,
      recodeflow::rec_with_table(
        data = get(
          load("../inst/extdata/bll-workflow/data/cchs2001_p.RData")
        ),
        database_name = "cchs2001_p",
        variables = variables_sheet,
        variable_details = variable_details_sheet
      )
    ),
    targets::tar_target(
      cchs2003_p_data,
      recodeflow::rec_with_table(
        data = get(
          load("../inst/extdata/bll-workflow/data/cchs2003_p.RData")
        ),
        database_name = "cchs2003_p",
        variables = variables_sheet,
        variable_details = variable_details_sheet
      )
    ),
    targets::tar_target(
      cchs2005_p_data,
      recodeflow::rec_with_table(
        data = get(
          load("../inst/extdata/bll-workflow/data/cchs2005_p.RData")
        ),
        database_name = "cchs2005_p",
        variables = variables_sheet,
        variable_details = variable_details_sheet
      )
    ),
    # Finally we will combine the three datasets into the one for the study
    # and remove all the missing from the outcome
    targets::tar_target(
      study_dataset,
      dplyr::bind_rows(cchs2001_p_data, cchs2003_p_data, cchs2005_p_data) %>%
        # This part removes all the missing from the outcome. Notice that we
        # don't refer to the outcome by its name (CCC_101), we use the "outcome"
        # role we defined in the variables sheet and the
        # recodeflow:::select_vars_by_role function to retreive the variable
        # tied to that role from the variables sheet.
        dplyr::filter(
          dplyr::across(
            recodeflow:::select_vars_by_role("outcome", variables_sheet),
            ~ .x != "NA(b)"
          )
        )
    )
  )
  list(import_worksheets, create_study_dataset)
},
ask = FALSE)

targets::tar_make()
```

We can view the study dataset using the code below.

```{r}
#DT::datatable(targets::tar_read("study_dataset"))
summary(targets::tar_read("study_dataset")$DHHGAGE_cont)
```

Notice how we did not refer to the outcome by its name but used the combination of roles and the recodeflow::select_vars_by_role function to refer to it. This allows us to separate the part of the study where we select all the variables in the study by shifting it to the variables sheet and the part of the study where we actually implement the analysis. Libraries like dplyr make it really easy to implement this concept hence its usage in the code above.

Lets further cement this concept by looking at the descriptives for our predictors. These statistics would normally go into a table 1 in a paper. For the continuous predictors this would be the mean, minimum, and maximum while for the categorical predictors this would be the proportion and number of individuals in each category.

```{r Descriptive data}
targets::tar_script({
  library(magrittr)
  library(dplyr)
  import_worksheets <- list(
    targets::tar_target(
      variable_details_sheet_file,
      "../inst/extdata/bll-workflow/variable_details.csv",
      format = "file"
    ),
    targets::tar_target(
      variable_details_sheet,
      read.csv(variable_details_sheet_file, fileEncoding = "UTF-8-BOM")
    ),
    targets::tar_target(
      variables_sheet_file,
      "../inst/extdata/bll-workflow/variables.csv",
      format = "file"
    ),
    targets::tar_target(
      variables_sheet,
      read.csv(variables_sheet_file, fileEncoding = "UTF-8-BOM")
    )
  )
  create_study_dataset <- list(
    targets::tar_target(
      cchs2001_p_data,
      recodeflow::rec_with_table(
        data = get(
          load("../inst/extdata/bll-workflow/data/cchs2001_p.RData")
        ),
        database_name = "cchs2001_p",
        variables = variables_sheet,
        variable_details = variable_details_sheet
      )
    ),
    targets::tar_target(
      cchs2003_p_data,
      recodeflow::rec_with_table(
        data = get(
          load("../inst/extdata/bll-workflow/data/cchs2003_p.RData")
        ),
        database_name = "cchs2003_p",
        variables = variables_sheet,
        variable_details = variable_details_sheet
      )
    ),
    targets::tar_target(
      cchs2005_p_data,
      recodeflow::rec_with_table(
        data = get(
          load("../inst/extdata/bll-workflow/data/cchs2005_p.RData")
        ),
        database_name = "cchs2005_p",
        variables = variables_sheet,
        variable_details = variable_details_sheet
      )
    ),
    targets::tar_target(
      study_dataset,
      dplyr::bind_rows(cchs2001_p_data, cchs2003_p_data, cchs2005_p_data) %>%
        dplyr::filter(
          dplyr::across(
            recodeflow:::select_vars_by_role("outcome", variables_sheet),
            ~ .x != "NA(b)"
          )
        )
    )
  )
  # This function will create the data frame with the descriptive data for
  # all variables in the variables argument
  get_descriptive_data <- function(
    # The study dataset
    data,
    # The variable names whose descriptive data we need
    variables,
    # The variables sheet for the study
    variables_sheet,
    # The variable details sheet for the study
    variable_details_sheet
  ) {
    # The data frame that will hold the descriptive data. We will append rows
    # of descriptive data throughout this function
    descriptive_data <- data.frame(
      # The variable name
      variable = c(),
      # The mean. Only for continuous variables
      mean = c(),
      # The minimim value. Only for continuous variables.
      min = c(),
      # The maximum value. Only for continuous variables.
      max = c(),
      # The category value for this variable. Only for categorical variables.
      cat = c(),
      # The proportion (value between 0 and 1) of individuals in this
      # category. Only for categorical variables.
      proportion = c(),
      # The number of individuals in this category. Only for categorical variables
      n = c()
    )
    # Go through each variable, calculate its descriptive data, and add it to
    # the above variable
    for(variable in variables) {
      variables_sheet_row <- variables_sheet[variables_sheet$variable == variable, ]
      # If the variable is continuous
      if(variables_sheet_row$variableType == "Continuous") {
        # Calculate its descriptive data
        desc_statistics <- summary(data[[variable]])
        # Add it to the descriptive data data frame
        descriptive_data <- rbind(
          descriptive_data,
          data.frame(
            variable = c(variable),
            mean = c(desc_statistics[4]),
            min = c(desc_statistics[1]),
            max = c(desc_statistics[6]),
            # The next three values are NA since this is a continuous variable
            cat = c(NA),
            proportion = c(NA),
            n = c(NA)
          )
        )
      }
      # Otherwise its a categorical variable
      else {
        # Get the variable details rows for this variable since it will have all
        # the categories for this categorical variable in this dataset 
        variable_details_rows <- variable_details_sheet[variable_details_sheet$variable == variable, ]
        variable_categories <- unique(variable_details_rows$recEnd)
        # Go through each category and add their descriptive data
        for(variable_category in variable_categories) {
          # The rows from the dataset for this variable for the current category
          variable_category_data <- data[data[[variable]] == variable_category, ]
          descriptive_data <- rbind(
            descriptive_data,
            data.frame(
              variable = c(variable),
              # The next three values are NA since this is a categorival variable
              mean = c(NA),
              min = c(NA),
              max = c(NA),
              cat = c(variable_category),
              proportion = c(nrow(variable_category_data)/nrow(data)),
              n = c(nrow(variable_category_data))
            )
          )
        }
      }
    }
    return(descriptive_data)
  }
  # The module to create the descriptive data
  create_descriptive_data <- list(
    targets::tar_target(
      descriptive_data,
      # Call the function declared above to do this
      get_descriptive_data(
        study_dataset,
        # We only want the descriptive data for the predictors, so use the role
        # predictor
        recodeflow:::select_vars_by_role("predictor", variables_sheet),
        variables_sheet,
        variable_details_sheet
      )
    )
  )
  list(import_worksheets, create_study_dataset, create_descriptive_data)
},
ask = FALSE)

targets::tar_make()
```

Once again, we did not refer to any variables by name in the code, but used the role predictor to select all the variables we want to be part of the descriptive table. We could have also done this by adding a new role to the variables sheet called "table-1" and using that in the code. Notice that we also created a function called `get_descriptive_data` which handles all the logic for actually creating the desciptive data, whereas in the step we are simply calling that function with the right arguments. This new function can now be easily reused in all studies that use this workflow. Finally, the data frame returned by the function would not be included in the paper directly by it easily be made publication ready by using a library like `kable` or `flextable`. This way we seperate the logic for generating the difference analysis data and the logic for presenting the data which will probably look different depending on the presentation mode.

The calculated descriptive data can be seen below.

```{r Display descriptive data}
DT::datatable(targets::tar_read("descriptive_data"))
```

Finally, lets fit our logistic regression model. We will once again use a function which handles all the fitting logic, and use roles to select the outcome and predictor variables.

```{r Model fit}
targets::tar_script({
  library(magrittr)
  library(dplyr)
  import_worksheets <- list(
    targets::tar_target(
      variable_details_sheet_file,
      "../inst/extdata/bll-workflow/variable_details.csv",
      format = "file"
    ),
    targets::tar_target(
      variable_details_sheet,
      read.csv(variable_details_sheet_file, fileEncoding = "UTF-8-BOM")
    ),
    targets::tar_target(
      variables_sheet_file,
      "../inst/extdata/bll-workflow/variables.csv",
      format = "file"
    ),
    targets::tar_target(
      variables_sheet,
      read.csv(variables_sheet_file, fileEncoding = "UTF-8-BOM")
    )
  )
  create_study_dataset <- list(
    targets::tar_target(
      cchs2001_p_data,
      recodeflow::rec_with_table(
        data = get(
          load("../inst/extdata/bll-workflow/data/cchs2001_p.RData")
        ),
        database_name = "cchs2001_p",
        variables = variables_sheet,
        variable_details = variable_details_sheet
      )
    ),
    targets::tar_target(
      cchs2003_p_data,
      recodeflow::rec_with_table(
        data = get(
          load("../inst/extdata/bll-workflow/data/cchs2003_p.RData")
        ),
        database_name = "cchs2003_p",
        variables = variables_sheet,
        variable_details = variable_details_sheet
      )
    ),
    targets::tar_target(
      cchs2005_p_data,
      recodeflow::rec_with_table(
        data = get(
          load("../inst/extdata/bll-workflow/data/cchs2005_p.RData")
        ),
        database_name = "cchs2005_p",
        variables = variables_sheet,
        variable_details = variable_details_sheet
      )
    ),
    targets::tar_target(
      study_dataset,
      dplyr::bind_rows(cchs2001_p_data, cchs2003_p_data, cchs2005_p_data) %>%
        dplyr::filter(
          dplyr::across(
            recodeflow:::select_vars_by_role("outcome", variables_sheet),
            ~ .x != "NA(b)"
          )
        )
    )
  )
  get_descriptive_data <- function(
    data,
    variables,
    variables_sheet,
    variable_details_sheet
  ) {
    descriptive_data <- data.frame(

      variable = c(),
      mean = c(),
      min = c(),
      max = c(),
      cat = c(),
      proportion = c(),
      n = c()
    )
    for(variable in variables) {
      variables_sheet_row <- variables_sheet[variables_sheet$variable == variable, ]
      if(variables_sheet_row$variableType == "Continuous") {
        desc_statistics <- summary(data[[variable]])
        descriptive_data <- rbind(
          descriptive_data,
          data.frame(
            variable = c(variable),
            mean = c(desc_statistics[4]),
            min = c(desc_statistics[1]),
            max = c(desc_statistics[6]),
            cat = c(NA),
            proportion = c(NA),
            n = c(NA)
          )
        )
      }
      else {
        variable_details_rows <- variable_details_sheet[variable_details_sheet$variable == variable, ]
        variable_categories <- unique(variable_details_rows$recEnd)
        for(variable_category in variable_categories) {
          variable_category_data <- data[data[[variable]] == variable_category, ]
          descriptive_data <- rbind(
            descriptive_data,
            data.frame(
              variable = c(variable),
              mean = c(NA),
              min = c(NA),
              max = c(NA),
              cat = c(variable_category),
              proportion = c(nrow(variable_category_data)/nrow(data)),
              n = c(nrow(variable_category_data))
            )
          )
        }
      }
    }
    return(descriptive_data)
  }
  create_descriptive_data <- list(
    targets::tar_target(
      descriptive_data,
      get_descriptive_data(
        study_dataset,
        recodeflow:::select_vars_by_role("predictor", variables_sheet),
        variables_sheet,
        variable_details_sheet
      )
    )
  )
  # This function will be used to fit the final logistic regression for this
  # study
  fit_model <- function(
    # The study dataset
    data, 
    # The name of the outcome variable
    outcome,
    # The names of the predictors
    predictors
  ) {
    # Create the formula for the model
    model_formula <- as.formula(paste(
      outcome, "~", paste(predictors, collapse = "+")
    ))
    # Fit the model
    model_fit <- glm(
      model_formula,
      data = data,
      family = binomial
    )
    return(model_fit)
  }
  model_fit <- list(
    targets::tar_target(
      model,
      fit_model(
        study_dataset,
        recodeflow:::select_vars_by_role("outcome", variables_sheet),
        recodeflow:::select_vars_by_role("predictor", variables_sheet)
      )
    )
  )
  list(import_worksheets, create_study_dataset, create_descriptive_data, model_fit)
},
ask = FALSE)

targets::tar_make()
```

And we can see the results of the model fit below,

```{r View model}
print(targets::tar_read("model"))
```

To recap,

* We introduced the libraries and methods to follow in the BLL workflow as well as the reasons for using the workflow
* We saw the use for the role column in the variables sheet and how to use it to avoid harcoding variable names in our analysis code. Using roles also makes it easier to add/remove variables from our analysis. Where before we would have to go through our code and do a find/replace, now we can just update the variables sheet and run our code again.
* We saw the importance of putting most of our analysis logic inside functions, which makes it easier to make portions of our analysis reusable across studies. The targets libraries in fact sometimes makes it impossible to write complicated steps without functions which is a useful side effect of its API.